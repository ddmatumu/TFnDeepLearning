{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETNLP: exploring different word embeddings\n",
    "Given many word embedding models, how do you know which one to use? Is it possible to have preliminary evaluations to predict which models to use for a certain downstream task? ETNLP will be a convenient tool for this purpose.\n",
    "\n",
    "- Readmore in this paper: https://arxiv.org/abs/1903.04433 of Xuan-Son Vu, Thanh Vu, Son N. Tran, Lili Jiang.\n",
    "\n",
    "## There are some TODOs in this Notebook:\n",
    "- TODO#1: (as always) read the codes and comments from begining to the end.\n",
    "- TODO#2: extract new set of embeddings based on new documents.\n",
    "- TODO#3: visualize to see the extracted embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonvx/anaconda2/envs/ipykernel_py3_ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import etnlp_api\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include *.py files from other folders\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlibs.rnn.data import dataset_sentiment_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset_sentiment_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"All good so far. I was incredibly hesitant to buy a refurbished phone on Amazon after reading so many negative reviews on various products. I spent a lot of time researching my options and figured this one was probably my safest bet. Phone came in perfect condition (seriously, it looks brand new) and so far all the functions seem great. Set up was easy, unlocked, ready to set my fingerprint and everything. It's fast, sleek, and beautiful. Camera and audio are also great.\\nDEFECTIVE BATTERY - The phone was defective the day it arrived. The battery does not hold a charge consistently or indicate how long it will actually last. This defect is sporadic—so sometimes it works and sometimes it doesn't. I thought I was doing something wrong at first so, after a few weeks of frustration, I did a test to see: tracking how long I charged and how long it lasted. This confirmed it was not my fault or a defective charger and that the phone I purchased from Electonic Deals was the problem.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_arr = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good so far.\n",
      "I was incredibly hesitant to buy a refurbished phone on Amazon after reading so many negative reviews on various products.\n",
      "I spent a lot of time researching my options and figured this one was probably my safest bet.\n",
      "Phone came in perfect condition (seriously, it looks brand new) and so far all the functions seem great.\n",
      "Set up was easy, unlocked, ready to set my fingerprint and everything.\n",
      "It's fast, sleek, and beautiful.\n",
      "Camera and audio are also great.\n",
      "DEFECTIVE BATTERY - The phone was defective the day it arrived.\n",
      "The battery does not hold a charge consistently or indicate how long it will actually last.\n",
      "This defect is sporadic—so sometimes it works and sometimes it doesn't.\n",
      "I thought I was doing something wrong at first so, after a few weeks of frustration, I did a test to see: tracking how long I charged and how long it lasted.\n",
      "This confirmed it was not my fault or a defective charger and that the phone I purchased from Electonic Deals was the problem.\n"
     ]
    }
   ],
   "source": [
    "for sen in sent_arr:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vocab list to extract embeddings\n",
    "from collections import Counter\n",
    "word_counter = Counter()\n",
    "for s in sent_arr:\n",
    "    word_counter.update(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(len(word_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'good', 'so', 'far.', 'I', 'was', 'incredibly', 'hesitant', 'to', 'buy']\n"
     ]
    }
   ],
   "source": [
    "print(list(word_counter.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/OProjects/DeepLearning/HPC2N/TFnDeepLearning/codes/jupyterlabs/day1\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"../../../data/etnlp/senti_vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/etnlp/senti_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls $vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get embeddings of these vectors only:\n",
    "# write vocab file first:\n",
    "fwriter = open(vocab_file, \"w\")\n",
    "for word in word_counter.keys():\n",
    "    fwriter.write(word + \"\\n\")\n",
    "fwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02. Extracting word embeddings ...\n",
      "Reading embedding file (may take a while)\n",
      "model_paths_list =  ['/mnt/data/PretrainedEmbeddings/EN/NER_fasttext_wiki_subword.vec']\n",
      "model_formats_list =  ['word2vec']\n",
      "Warning: char_model is None -> cannot solve OOV word. Keep going ...\n",
      "- At line 0\n",
      "- done. Found 85 vectors for 120 words\n",
      "Done\n",
      "02. Extracting word embeddings ...\n",
      "Reading embedding file (may take a while)\n",
      "model_paths_list =  ['/mnt/data/PretrainedEmbeddings/EN/NER_fasttext_wiki.vec']\n",
      "model_formats_list =  ['word2vec']\n",
      "Warning: char_model is None -> cannot solve OOV word. Keep going ...\n",
      "- At line 0\n",
      "- done. Found 85 vectors for 120 words\n",
      "Done\n",
      "02. Extracting word embeddings ...\n",
      "Reading embedding file (may take a while)\n",
      "model_paths_list =  ['/mnt/data/PretrainedEmbeddings/EN/NER_glove.840B.300d.vec']\n",
      "model_formats_list =  ['word2vec']\n",
      "Warning: char_model is None -> cannot solve OOV word. Keep going ...\n",
      "- At line 0\n",
      "- done. Found 85 vectors for 120 words\n",
      "Done\n",
      "02. Extracting word embeddings ...\n",
      "Reading embedding file (may take a while)\n",
      "model_paths_list =  ['/mnt/data/PretrainedEmbeddings/EN/NER_W2V_google_news.vec']\n",
      "model_formats_list =  ['word2vec']\n",
      "Warning: char_model is None -> cannot solve OOV word. Keep going ...\n",
      "- At line 0\n",
      "- done. Found 80 vectors for 120 words\n",
      "Done\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Load embedding models and write down:\n",
    "from etnlp_api import embedding_config\n",
    "from etnlp_api import embedding_extractor\n",
    "\n",
    "# You don't need to run this part, just for demonstration the whole process.\n",
    "def do_extraction():\n",
    "    emb1 = \"/mnt/data/PretrainedEmbeddings/EN/NER_fasttext_wiki_subword.vec\"\n",
    "    emb2 = \"/mnt/data/PretrainedEmbeddings/EN/NER_fasttext_wiki.vec\"\n",
    "    emb3 = \"/mnt/data/PretrainedEmbeddings/EN/NER_glove.840B.300d.vec\"\n",
    "    emb4 = \"/mnt/data/PretrainedEmbeddings/EN/NER_W2V_google_news.vec\"\n",
    "    C2V = None\n",
    "    out1 = \"../../../data/etnlp/Tiny_fasttext_wiki_subword.vec\"\n",
    "    out2 = \"../../../data/etnlp/Tiny_fasttext_wiki.vec\"\n",
    "    out3 = \"../../../data/etnlp/Tiny_glove.840B.300d.vec\"\n",
    "    out4 = \"../../../data/etnlp/Tiny_W2V_google_news.vec\"\n",
    "\n",
    "    VOCAB_FILE = vocab_file\n",
    "    # OUTPUT_FORMAT=\".txt;.npz;.gz\"\n",
    "    OUTPUT_FORMAT = \".txt\"\n",
    "    # embedding_config\n",
    "    embedding_config.do_normalize_emb = False\n",
    "\n",
    "    emb_files = [emb1, emb2, emb3, emb4]\n",
    "    out_files = [out1, out2, out3, out4]\n",
    "\n",
    "    for emb_file, out_file in zip(emb_files, out_files):\n",
    "        embedding_extractor.extract_embedding_for_vocab_file(emb_file, VOCAB_FILE,\n",
    "                                                         C2V, out_file, OUTPUT_FORMAT)\n",
    "    print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/OProjects/DeepLearning/HPC2N/TFnDeepLearning/codes/jupyterlabs/day1\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003_ner_vocab.txt  Ti_ft_wi_sub.vec  Ti_gl.vec\n",
      "senti_vocab.txt\t\t Ti_ft_wi.vec\t   Ti_W2V.vec\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../../data/etnlp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from etnlp_api import embedding_visualizer\n",
    "out1 = \"../../../data/etnlp/Ti_ft_wi_sub.vec\"\n",
    "out2 = \"../../../data/etnlp/Ti_ft_wi.vec\"\n",
    "out3 = \"../../../data/etnlp/Ti_gl.vec\"\n",
    "out4 = \"../../../data/etnlp/Ti_W2V.vec\"\n",
    "\n",
    "INPUT_FILES = \"%s;%s;%s;%s\"%(out1, out2, out3, out4)\n",
    "\n",
    "# etnlp_api.embedding_visualizer.visualize_multiple_embeddings(INPUT_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Interactive visualization:\n",
    "Please run on Terminal tab, it won't work by !<command> on this Notebook.\n",
    "- 1. active the environment\n",
    "- 2. sh scripts/03.run_etnlp_visualizer_iter_local.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Side-by-side visualization:\n",
    "Please run on Terminal tab, it won't work by !<command> on this Notebook.\n",
    "- 1. active the environment\n",
    "- 2. sh scripts/04.run_etnlp_visualizer_sbs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation\n",
    "- See more at the github page: https://github.com/vietnlp/etnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TODOs:\n",
    "- TODO#2: extract new set of embeddings based on new documents.\n",
    "- TODO#3: visualize to see the extracted embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please go to bbc.com or any other website to copy and paste here \n",
    "# a piece of text (not too long, not too short). \n",
    "new_text = \"\"\"\n",
    "<PASTE YOUR TEXT HERE>\n",
    "\"\"\"\n",
    "\n",
    "# Next: please replicate the whole process again with this new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = \"../../../data/etnlp/Ti_ft_wi_sub.vec\"\n",
    "in2 = \"../../../data/etnlp/Ti_ft_wi.vec\"\n",
    "in3 = \"../../../data/etnlp/Ti_gl.vec\"\n",
    "in4 = \"../../../data/etnlp/Ti_W2V.vec\"\n",
    "\n",
    "# TODO HERE: the same with the do_extraction() function but with the above 4 in_embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions: after this Notebook, you know:\n",
    "- How to extract new embeddings for a new downstream tasks.\n",
    "- How to visualizing different embeddings to explore them interactively or compare them side-by-side.\n",
    "- How to have preliminary evaluations to have a better judgment which embeddings to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
