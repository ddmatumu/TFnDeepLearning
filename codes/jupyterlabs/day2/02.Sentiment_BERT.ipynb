{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with BERT\n",
    "\n",
    "We want to predict sentiment from a IMDB movie review dataset. For every review we take only the first 100 words - we want in fact to check wheter most of the actual sentiment is at the beginning of the reviews, rather than at the end.\n",
    "\n",
    "In this dataset, sentiment is either positive or negative, 0 or 1. (There exists also a version with the 1 to 10 scale of votes)\n",
    "\n",
    "- [BERT code](https://github.com/google-research/bert)\n",
    "\n",
    "\n",
    "# In this Notebook, you're going to finish some following #TODOs \n",
    "\n",
    "- Try to understand how a big model such as BERT is handled in Tensorflow... without tensorflow hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "def add_path(path):\n",
    "    module_path = os.path.abspath(os.path.join(path))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "        \n",
    "    [add_path(x[0]) for x in os.walk(path) if x[0] != path]\n",
    "    \n",
    "\n",
    "add_path('../../pythonlibs/embeddings')\n",
    "add_path('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import zipfile\n",
    "from bert.helper import get_tpu_estimator, imdb_to_bert_features\n",
    "import bert.run_classifier as run_classifier\n",
    "from imdb.helper import get_imdb_reviews_dataset\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download the bert model and save it to the specified **bert_repo_path**.\n",
    "\n",
    "These files contain the architecture and the weights of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_repo_path = '../../pythonlibs/embeddings/bert/model_repo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "    \n",
    "\n",
    "with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(bert_repo_path)\n",
    "\n",
    "!ls 'model_repo/uncased_L-12_H-768_A-12'\n",
    "\n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load IMDB dataset from the specified path\n",
    "\n",
    "#### Additionally, the dataset must be converted into word+position+phrase embeddings before BERT can process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../../../data/aclImdb'\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = get_imdb_reviews_dataset(path=dataset_folder, max_dataset_size=1000, trunc=100)\n",
    "\n",
    "\n",
    "max_seq_length = 150\n",
    "\n",
    "train_features = imdb_to_bert_features(np.hstack([train_x, train_y]), max_seq_length, repo_path=bert_repo_path, train=True)\n",
    "test_features  = imdb_to_bert_features(np.hstack([test_x, test_y]), max_seq_length, repo_path=bert_repo_path, train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Specify a classifier to be concatenated to BERT\n",
    "We will use a pre-trained model BERT and put on top of it a classifier. This classifier will thus use BERT's output features to make its decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn(bert_output, labels, num_labels, is_training=True):\n",
    "    '''\n",
    "    bert_output: BERT's output tensor\n",
    "    labels: labels tensor\n",
    "    num_labels: number of labels (used to size the tensor)\n",
    "    is_training: True if the model is created for training.\n",
    "    '''\n",
    "    \n",
    "    output_layer = bert_output\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "                            name=\"output_weights\", \n",
    "                            shape=[num_labels, hidden_size] \n",
    "                     )\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "                            name=\"output_bias\", \n",
    "                            shape=[num_labels]\n",
    "                  )\n",
    "\n",
    "    if is_training:\n",
    "        # I.e., 0.1 dropout\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias, name='output_logits')\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "\n",
    "    return per_example_loss, logits, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the classifier and BERT\n",
    "\n",
    "\n",
    "A [tensorflow estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\n",
    "contains all of the procedures to perform training. It's a nice way to pack a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 10\n",
    "\n",
    "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(len(train_features)))\n",
    "print('  Batch size = {}'.format(batch_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "After instantiating BERT from the relative repository, get_tpu_estimator will create and stack on top of it the classifier.\n",
    "Finally, everything gets packed inside an estimator and returned.\n",
    "'''\n",
    "\n",
    "\n",
    "estimator = get_tpu_estimator(bert_repo_path, epochs, len(train_features), batch_size, 2, classifier_model_fn)\n",
    "\n",
    "\n",
    "# An estimator must be fed with a dataset a batch at a time. \n",
    "# This will create the function that will feed the model.\n",
    "\n",
    "train_input_fn = run_classifier.input_fn_builder(\n",
    "                        features=train_features,\n",
    "                        seq_length=max_seq_length,\n",
    "                        is_training=True,\n",
    "                        drop_remainder=True)\n",
    "\n",
    "\n",
    "# Train the estimator with the feeding function.\n",
    "estimator.train(input_fn=train_input_fn)\n",
    "\n",
    "print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions\n",
    "\n",
    "In this Notebook we saw how to use BERT for a custom classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
