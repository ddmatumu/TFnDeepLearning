{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow and Recurrent Neural Networks\n",
    "In this Notebook, you will learn how to train a language model to generate a sentence given a few words (very similar to the mighty GPT-2 model. It's cool, right? :D).\n",
    "\n",
    "## There are some TODOs in this Notebook:\n",
    "- TODO#1: (as always) read the codes and comments from the begining to the end.\n",
    "- TODO#2: train the dataset to a bigger one, with more documents.\n",
    "- TODO#3: add Tensorboard to visualize the graph and see training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonvx/anaconda2/envs/ipykernel_py3_ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import sys\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include *.py files from other folders\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlibs.rnn.data import dataset_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['All', 'good', 'so', 'far.', 'I', 'was', 'incredibly', 'hesitant',\n",
       "       'to', 'buy', 'a', 'refurbished', 'phone', 'on', 'Amazon', 'after',\n",
       "       'reading', 'so', 'many', 'negative', 'reviews', 'on', 'various',\n",
       "       'products.', 'I', 'spent', 'a', 'lot', 'of', 'time', 'researching',\n",
       "       'my', 'options', 'and', 'figured', 'this', 'one', 'was',\n",
       "       'probably', 'my', 'safest', 'bet.', 'Phone', 'came', 'in',\n",
       "       'perfect', 'condition', '(seriously,', 'it', 'looks', 'brand',\n",
       "       'new)', 'and', 'so', 'far', 'all', 'the', 'functions', 'seem',\n",
       "       'great.', 'Set', 'up', 'was', 'easy,', 'unlocked,', 'ready', 'to',\n",
       "       'set', 'my', 'fingerprint', 'and', 'everything.', \"It's\", 'fast,',\n",
       "       'sleek,', 'and', 'beautiful.', 'Camera', 'and', 'audio', 'are',\n",
       "       'also', 'great.', '1.', 'DEFECTIVE', 'BATTERY', '-', 'The',\n",
       "       'phone', 'was', 'defective', 'the', 'day', 'it', 'arrived.', 'The',\n",
       "       'battery', 'does', 'not', 'hold', 'a', 'charge', 'consistently',\n",
       "       'or', 'indicate', 'how', 'long', 'it', 'will', 'actually', 'last.',\n",
       "       'This', 'defect', 'is', 'sporadic—so', 'sometimes', 'it', 'works',\n",
       "       'and', 'sometimes', 'it', \"doesn't.\", 'I', 'thought', 'I', 'was',\n",
       "       'doing', 'something', 'wrong', 'at', 'first', 'so,', 'after', 'a',\n",
       "       'few', 'weeks', 'of', 'frustration,', 'I', 'did', 'a', 'test',\n",
       "       'to', 'see:', 'tracking', 'how', 'long', 'I', 'charged', 'and',\n",
       "       'how', 'long', 'it', 'lasted.', 'This', 'confirmed', 'it', 'was',\n",
       "       'not', 'my', 'fault', 'or', 'a', 'defective', 'charger', 'and',\n",
       "       'that', 'the', 'phone', 'I', 'purchased', 'from', 'Electonic',\n",
       "       'Deals', 'was', 'the', 'problem.'], dtype='<U12')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, reverse_dictionary = build_dataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location to write log files.\n",
    "logs_path = '../../my_data/tf_rnn_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_input = 3 # 3 symbols are retrieved from the text\n",
    "n_hidden = 100 # 10\n",
    "vocab_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the RNN graph\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    \"out\" : tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"out\" : tf.Variable(tf.random_normal([vocab_size]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    x = tf.split(x, n_input, 1)\n",
    "    \n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "    # rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "    \n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    \n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ef87c435aa86>:18: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "training_iters = 200\n",
    "pred = RNN(x, weights, biases)\n",
    "offset = random.randint(0,n_input+1)\n",
    "end_offset = n_input + 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-39f32152971b>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Model evaluator\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 5, Average Loss= 6.418518, Average Accuracy= 0.00%\n",
      "['negative', 'reviews', 'on'] - [various] vs [fault]\n",
      "Iter= 10, Average Loss= 5.216174, Average Accuracy= 0.00%\n",
      "['my', 'safest', 'bet.'] - [Phone] vs [researching]\n",
      "Iter= 15, Average Loss= 6.223140, Average Accuracy= 0.00%\n",
      "['great.', 'Set', 'up'] - [was] vs [researching]\n",
      "Iter= 20, Average Loss= 6.167353, Average Accuracy= 0.00%\n",
      "['audio', 'are', 'also'] - [great.] vs [researching]\n",
      "Iter= 25, Average Loss= 5.631999, Average Accuracy= 0.00%\n",
      "['hold', 'a', 'charge'] - [consistently] vs [researching]\n",
      "Iter= 30, Average Loss= 6.369317, Average Accuracy= 0.00%\n",
      "['sometimes', 'it', \"doesn't.\"] - [I] vs [defective]\n",
      "Iter= 35, Average Loss= 5.349265, Average Accuracy= 0.00%\n",
      "['did', 'a', 'test'] - [to] vs [defective]\n",
      "Iter= 40, Average Loss= 5.121295, Average Accuracy= 0.00%\n",
      "['my', 'fault', 'or'] - [a] vs [defective]\n",
      "Iter= 45, Average Loss= 4.834794, Average Accuracy= 0.00%\n",
      "['I', 'was', 'incredibly'] - [hesitant] vs [defective]\n",
      "Iter= 50, Average Loss= 6.011782, Average Accuracy= 0.00%\n",
      "['I', 'spent', 'a'] - [lot] vs [defective]\n",
      "Iter= 55, Average Loss= 6.192660, Average Accuracy= 0.00%\n",
      "['in', 'perfect', 'condition'] - [(seriously,] vs [that]\n",
      "Iter= 60, Average Loss= 5.487277, Average Accuracy= 0.00%\n",
      "['unlocked,', 'ready', 'to'] - [set] vs [or]\n",
      "Iter= 65, Average Loss= 5.382868, Average Accuracy= 0.00%\n",
      "['DEFECTIVE', 'BATTERY', '-'] - [The] vs [and]\n",
      "Iter= 70, Average Loss= 5.933173, Average Accuracy= 0.00%\n",
      "['indicate', 'how', 'long'] - [it] vs [or]\n",
      "Iter= 75, Average Loss= 5.195337, Average Accuracy= 0.00%\n",
      "['I', 'was', 'doing'] - [something] vs [fault]\n",
      "Iter= 80, Average Loss= 5.435372, Average Accuracy= 0.00%\n",
      "['tracking', 'how', 'long'] - [I] vs [see:]\n",
      "Iter= 85, Average Loss= 5.250078, Average Accuracy= 0.00%\n",
      "['charger', 'and', 'that'] - [the] vs [sometimes]\n",
      "Iter= 90, Average Loss= 5.678815, Average Accuracy= 0.00%\n",
      "['refurbished', 'phone', 'on'] - [Amazon] vs [the]\n",
      "Iter= 95, Average Loss= 5.553179, Average Accuracy= 0.00%\n",
      "['my', 'options', 'and'] - [figured] vs [defective]\n",
      "Iter= 100, Average Loss= 5.643300, Average Accuracy= 0.00%\n",
      "['new)', 'and', 'so'] - [far] vs [doesn't.]\n",
      "Iter= 105, Average Loss= 5.690943, Average Accuracy= 0.00%\n",
      "['everything.', \"It's\", 'fast,'] - [sleek,] vs [condition]\n",
      "Iter= 110, Average Loss= 4.535628, Average Accuracy= 0.00%\n",
      "['the', 'day', 'it'] - [arrived.] vs [defective]\n",
      "Iter= 115, Average Loss= 5.145699, Average Accuracy= 0.00%\n",
      "['This', 'defect', 'is'] - [sporadic—so] vs [and]\n",
      "Iter= 120, Average Loss= 4.298618, Average Accuracy= 20.00%\n",
      "['so,', 'after', 'a'] - [few] vs [doing]\n",
      "Iter= 125, Average Loss= 4.917815, Average Accuracy= 0.00%\n",
      "['long', 'it', 'lasted.'] - [This] vs [long]\n",
      "Iter= 130, Average Loss= 4.725797, Average Accuracy= 0.00%\n",
      "['from', 'Electonic', 'Deals'] - [was] vs [how]\n",
      "Iter= 135, Average Loss= 5.190567, Average Accuracy= 0.00%\n",
      "['reading', 'so', 'many'] - [negative] vs [far.]\n",
      "Iter= 140, Average Loss= 5.484662, Average Accuracy= 0.00%\n",
      "['one', 'was', 'probably'] - [my] vs [that]\n",
      "Iter= 145, Average Loss= 5.149543, Average Accuracy= 0.00%\n",
      "['the', 'functions', 'seem'] - [great.] vs [that]\n",
      "Iter= 150, Average Loss= 5.533093, Average Accuracy= 0.00%\n",
      "['beautiful.', 'Camera', 'and'] - [audio] vs [and]\n",
      "Iter= 155, Average Loss= 5.565665, Average Accuracy= 0.00%\n",
      "['battery', 'does', 'not'] - [hold] vs [and]\n",
      "Iter= 160, Average Loss= 4.679450, Average Accuracy= 0.00%\n",
      "['it', 'works', 'and'] - [sometimes] vs [or]\n",
      "Iter= 165, Average Loss= 5.353205, Average Accuracy= 0.00%\n",
      "['of', 'frustration,', 'I'] - [did] vs [sometimes]\n",
      "Iter= 170, Average Loss= 4.350570, Average Accuracy= 0.00%\n",
      "['it', 'was', 'not'] - [my] vs [defective]\n",
      "Iter= 175, Average Loss= 4.822539, Average Accuracy= 0.00%\n",
      "['far.', 'I', 'was'] - [incredibly] vs [the]\n",
      "Iter= 180, Average Loss= 4.937268, Average Accuracy= 0.00%\n",
      "['products.', 'I', 'spent'] - [a] vs [the]\n",
      "Iter= 185, Average Loss= 5.326925, Average Accuracy= 0.00%\n",
      "['came', 'in', 'perfect'] - [condition] vs [sometimes]\n",
      "Iter= 190, Average Loss= 5.348317, Average Accuracy= 0.00%\n",
      "['easy,', 'unlocked,', 'ready'] - [to] vs [condition]\n",
      "Iter= 195, Average Loss= 4.367728, Average Accuracy= 0.00%\n",
      "['1.', 'DEFECTIVE', 'BATTERY'] - [-] vs [and]\n",
      "Iter= 200, Average Loss= 4.223486, Average Accuracy= 0.00%\n",
      "['or', 'indicate', 'how'] - [long] vs [arrived.]\n"
     ]
    }
   ],
   "source": [
    "# After reshaping to fit the feed dictionary, we run the optimization:\n",
    "init = tf.global_variables_initializer()\n",
    "display_step = 5\n",
    "\n",
    "session.run(init)\n",
    "step = 0\n",
    "offset = random.randint(0,n_input+1)\n",
    "end_offset = n_input + 1\n",
    "acc_total = 0\n",
    "loss_total = 0\n",
    "\n",
    "writer.add_graph(session.graph)\n",
    "\n",
    "while step < training_iters:\n",
    "    # Generate a minibatch. Add some randomness on selection process.\n",
    "    if offset > (len(training_data)-end_offset):\n",
    "        offset = random.randint(0, n_input+1)\n",
    "\n",
    "    symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "    symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "    symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "    symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "    symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "    _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                            feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "    loss_total += loss\n",
    "    acc_total += acc\n",
    "    if (step+1) % display_step == 0:\n",
    "        print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "              \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "              \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "        acc_total = 0\n",
    "        loss_total = 0\n",
    "        symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "        symbols_out = training_data[offset + n_input]\n",
    "        symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "        print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "    step += 1\n",
    "    offset += (n_input+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "3 words:  I was good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was good defective long long defective defective long long defective defective long long defective defective long long defective defective long long defective defective long long defective defective long long defective defective long long defective\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "3 words:  a long iPhone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception:  'iPhone'\n",
      "Word not in dictionary\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "3 words:  a long word\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception:  'word'\n",
      "Word not in dictionary\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "3 words:  a a long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a long defective defective long long defective defective long long defective defective long long defective defective long long defective defective long long defective defective long long defective defective long long defective defective long long\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "3 words:  exit\n"
     ]
    }
   ],
   "source": [
    "# Play with trained model:\n",
    "sentence = \"\"\n",
    "while sentence != \"exit\":\n",
    "    prompt = \"%s words: \" % n_input\n",
    "    sentence = input(prompt)\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    if len(words) != n_input:\n",
    "        continue\n",
    "    try:\n",
    "        symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "        for i in range(32):\n",
    "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "            symbols_in_keys = symbols_in_keys[1:]\n",
    "            symbols_in_keys.append(onehot_pred_index)\n",
    "        print(sentence)\n",
    "    except Exception as ex:\n",
    "        print(\"Exception: \", ex)\n",
    "        print(\"Word not in dictionary\")\n",
    "\n",
    "# Close Tensorflow session.\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "- TODO#2: train the dataset to a bigger one, with more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please go to bbc.com or any other website to copy and \n",
    "# paste here a piece of text (not too long, not too short). \n",
    "text_longer = \"\"\"\n",
    "<REPLACE WITH YOUR TEXT HERE>\n",
    "\"\"\"\n",
    "# Next: please replicate the whole process again with this new text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "- TODO#3: add Tensorboard to visualize the graph and see training parameters.<br>\n",
    "(You are recommended to hook the Tensorboard codes back into the above codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "After this Notebook, you should know:\n",
    "- How to build an RNN network for a language modelling task.\n",
    "- How to stack multiple RNN layers.\n",
    "- How to use an interactive session instead of the traditional session of Tensorflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
