{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02.a: Tensorboard Tutorial with CNN on MNIST dataset.\n",
    "\n",
    "This tutorial will guid you on how to use the Tensorboard. Tensorboard is an amazing utility that allows us to visualize data and how it behaves. In this tutorial, you will see for what sort of purposes you can use the Tensorboard when training a neural network. \n",
    "\n",
    "Please refer to this excellent [article](https://www.datacamp.com/community/tutorials/tensorboard-tutorial) for more information. In this course we only discuss the first part of it.\n",
    "\n",
    "## There are only one TODO in this Notebook:\n",
    "- TODO#1: (as always) read the codes and comments from begining to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonvx/anaconda2/envs/ipykernel_py3_ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json \n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions,labels):\n",
    "    '''\n",
    "    Accuracy of a given set of predictions of size (N x n_classes) and\n",
    "    labels of size (N x n_classes)\n",
    "    '''\n",
    "    return np.sum(np.argmax(predictions,axis=1)==np.argmax(labels,axis=1))*100.0/labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Inputs, Outputs, Weights and Biases\n",
    "\n",
    "First you define a `batch_size` denoting the amount of data you sample at a single optimization/validation or testing step. Then you define the `layer_ids`, which gives an identifier for each of the layers of the neural network you will be defining. You then can define `layer_sizes`. Note that `len(layer_sizes)` should be `len(layer_ids)+1`, because `layer_sizes` includes the size of the input at the beginning. MNIST has images of size 28x28, which will be 784 when unwrapped to a single dimension. Then you can define the input and label placeholders, that you will later use to train the model. Finally you define two TensorFlow variables for each layer (that is, `weights` and `bias`).\n",
    "\n",
    "You can use variable scoping (more information [here](https://www.tensorflow.org/programmers_guide/variables)) so that the variables will be nicely named and will be much easier to access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "layer_ids = ['hidden1','hidden2','hidden3','hidden4','hidden5','out']\n",
    "layer_sizes = [784, 500, 400, 300, 200, 100, 10]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs and Labels\n",
    "train_inputs = tf.placeholder(tf.float32, shape=[batch_size, layer_sizes[0]], name='train_inputs')\n",
    "train_labels = tf.placeholder(tf.float32, shape=[batch_size, layer_sizes[-1]], name='train_labels')\n",
    "\n",
    "# Weight and Bias definitions\n",
    "for idx, lid in enumerate(layer_ids):\n",
    "    \n",
    "    with tf.variable_scope(lid):\n",
    "        w = tf.get_variable('weights',shape=[layer_sizes[idx], layer_sizes[idx+1]], \n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "        b = tf.get_variable('bias',shape= [layer_sizes[idx+1]], \n",
    "                            initializer=tf.random_uniform_initializer(-0.1,0.1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Logits, Predictions, Loss and Optimization\n",
    "\n",
    "With the input/output placeholders, weights and biases of each layer defined, you now can define the calculations to calculate the logits of the neural network. Logits are the unnormalized values produced at the last layer of the neural network. When normalized, you call them predictions. This involves iterating through each layer in the neural network and computing `tf.matmul(h,w) +b`. You also need to apply an activation function as `tf.nn.relu(tf.matmul(h,w) +b)`, for all layers except for the last layer.\n",
    "\n",
    "Next you define loss function that is used to optimize the neural network. In this example, you can use the cross entropy loss, which often deliver better results in classification problems than the mean squared error.\n",
    "\n",
    "Finally you will need to define an optimizer that takes in the loss and update the weights of the neural network in the direction that minimizes the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating Logits\n",
    "h = train_inputs\n",
    "for lid in layer_ids:\n",
    "    with tf.variable_scope(lid,reuse=True):\n",
    "        w, b = tf.get_variable('weights'), tf.get_variable('bias')\n",
    "        if lid != 'out':\n",
    "            h = tf.nn.relu(tf.matmul(h,w)+b,name=lid+'_output')\n",
    "        else:\n",
    "            h = tf.nn.xw_plus_b(h,w,b,name=lid+'_output')\n",
    "\n",
    "tf_predictions = tf.nn.softmax(h, name='predictions')\n",
    "# Calculating Loss\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=train_labels, logits=h),name='loss')\n",
    "\n",
    "# Optimizer \n",
    "tf_learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "optimizer = tf.train.MomentumOptimizer(tf_learning_rate,momentum=0.9)\n",
    "grads_and_vars = optimizer.compute_gradients(tf_loss)\n",
    "tf_loss_minimize = optimizer.minimize(tf_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tensorboard Summaries\n",
    "\n",
    "Here you can define the `tf.summary` objects. `tf.summary` objects are the type of entities understood by the Tensorboard. This means that whatever value you'd like to be displayed on the Tensorboard, you should encapsulate it as a `tf.summary` object. There are several different types of summaries. Here as you are visualizing only scalars, you can define `tf.summary.scalar` objects. Furthermore, you can use `tf.name_scope` to group scalars on the Tensorboard. That is, scalars having the same name scope will be displayed on the same row on the Tensorboard. Here you define three different summaries.\n",
    "\n",
    "* `tf_loss_summary` : You feed in a value by means of a placeholder, whenever you need to publish this to the Tensorboard\n",
    "* `tf_accuracy_summary` : You feed in a value by means of a placeholder, whenever you need to publish this to the Tensorboard\n",
    "* `tf_gradnorm_summary` : This calculates the l2 norm of the gradients of the last layer of your neural network. Gradient norm is a good indicator of whether the weights of the neural network are being properly updated. A too small gradient norm can indicate *vanishing gradient* or a too large gradient can imply *exploding gradient* phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name scope allows you to group various summaries together\n",
    "# Summaries having the same name_scope will be displayed on the same row on the Tensorboard\n",
    "with tf.name_scope('performance'):\n",
    "    # Summaries need to display on the Tensorboard\n",
    "    # Whenever need to record the loss, feed the mean loss to this placeholder\n",
    "    tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary') \n",
    "    # Create a scalar summary object for the loss so Tensorboard knows how to display it\n",
    "    tf_loss_summary = tf.summary.scalar('loss', tf_loss_ph)\n",
    "\n",
    "    # Whenever you need to record the loss, feed the mean test accuracy to this placeholder\n",
    "    tf_accuracy_ph = tf.placeholder(tf.float32,shape=None, name='accuracy_summary') \n",
    "    # Create a scalar summary object for the accuracy so Tensorboard knows how to display it\n",
    "    tf_accuracy_summary = tf.summary.scalar('accuracy', tf_accuracy_ph)\n",
    "\n",
    "# Gradient norm summary\n",
    "for g,v in grads_and_vars:\n",
    "    if 'hidden5' in v.name and 'weights' in v.name:\n",
    "        with tf.name_scope('gradients'):\n",
    "            tf_last_grad_norm = tf.sqrt(tf.reduce_mean(g**2))\n",
    "            tf_gradnorm_summary = tf.summary.scalar('grad_norm', tf_last_grad_norm)\n",
    "            break\n",
    "# Merge all summaries together\n",
    "performance_summaries = tf.summary.merge([tf_loss_summary,tf_accuracy_summary])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the neural network model: Loading Data, Training, Validation and Testing\n",
    "\n",
    "In the code below you do the following. First you create a session, in which you execute the operations you defined above. Then you create folder for saving summary data. You next create a summary write `summ_writer`. You can now initialize all variables. This will be followed by loading the MNIST dataset.\n",
    "\n",
    "Then for each epoch, and each batch in training data (that is, each iteration). Execute `gradnorm_summary` if it is the first iteration and write `gradnorm_summary` to event file with summary writer. You now execute model optimization and calculating the loss. After you go through the full training dataset for a single epoch, calculate average training loss.\n",
    "\n",
    "You follow a similar treatment for the validation dataset as well. Specifically, for each batch in validation data, you calculate validation accuracy for each batch. Thereafter calculate average validation accuracy for full validation set.\n",
    "\n",
    "Finally, the testing phase is executed. In this, for each batch in test data, you calculate test accuracy for each batch. With that, you calculate average test accuracy for full test set. At the very end you execute `performance_summaries` and write them to event file with the summary writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-f67ba15e6a2a>:26: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/sonvx/anaconda2/envs/ipykernel_py3_ml/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/sonvx/anaconda2/envs/ipykernel_py3_ml/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sonvx/anaconda2/envs/ipykernel_py3_ml/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sonvx/anaconda2/envs/ipykernel_py3_ml/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sonvx/anaconda2/envs/ipykernel_py3_ml/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Average loss in epoch 0: 2.30306\n",
      "\tAverage Valid Accuracy in epoch 0: 11.26000\n",
      "\tAverage Test Accuracy in epoch 0: 11.35000\n",
      "\n",
      "Average loss in epoch 1: 2.30125\n",
      "\tAverage Valid Accuracy in epoch 1: 11.26000\n",
      "\tAverage Test Accuracy in epoch 1: 11.35000\n",
      "\n",
      "Average loss in epoch 2: 2.29951\n",
      "\tAverage Valid Accuracy in epoch 2: 11.26000\n",
      "\tAverage Test Accuracy in epoch 2: 11.35000\n",
      "\n",
      "Average loss in epoch 3: 2.29775\n",
      "\tAverage Valid Accuracy in epoch 3: 11.26000\n",
      "\tAverage Test Accuracy in epoch 3: 11.35000\n",
      "\n",
      "Average loss in epoch 4: 2.29591\n",
      "\tAverage Valid Accuracy in epoch 4: 11.26000\n",
      "\tAverage Test Accuracy in epoch 4: 11.35000\n",
      "\n",
      "Average loss in epoch 5: 2.29392\n",
      "\tAverage Valid Accuracy in epoch 5: 11.26000\n",
      "\tAverage Test Accuracy in epoch 5: 11.35000\n",
      "\n",
      "Average loss in epoch 6: 2.29170\n",
      "\tAverage Valid Accuracy in epoch 6: 11.30000\n",
      "\tAverage Test Accuracy in epoch 6: 11.44000\n",
      "\n",
      "Average loss in epoch 7: 2.28919\n",
      "\tAverage Valid Accuracy in epoch 7: 12.30000\n",
      "\tAverage Test Accuracy in epoch 7: 12.38000\n",
      "\n",
      "Average loss in epoch 8: 2.28631\n",
      "\tAverage Valid Accuracy in epoch 8: 14.42000\n",
      "\tAverage Test Accuracy in epoch 8: 14.39000\n",
      "\n",
      "Average loss in epoch 9: 2.28293\n",
      "\tAverage Valid Accuracy in epoch 9: 16.92000\n",
      "\tAverage Test Accuracy in epoch 9: 16.46000\n",
      "\n",
      "Average loss in epoch 10: 2.27883\n",
      "\tAverage Valid Accuracy in epoch 10: 18.68000\n",
      "\tAverage Test Accuracy in epoch 10: 18.46000\n",
      "\n",
      "Average loss in epoch 11: 2.27376\n",
      "\tAverage Valid Accuracy in epoch 11: 20.38000\n",
      "\tAverage Test Accuracy in epoch 11: 21.03000\n",
      "\n",
      "Average loss in epoch 12: 2.26755\n",
      "\tAverage Valid Accuracy in epoch 12: 23.92000\n",
      "\tAverage Test Accuracy in epoch 12: 25.04000\n",
      "\n",
      "Average loss in epoch 13: 2.25983\n",
      "\tAverage Valid Accuracy in epoch 13: 29.38000\n",
      "\tAverage Test Accuracy in epoch 13: 31.18000\n",
      "\n",
      "Average loss in epoch 14: 2.24985\n",
      "\tAverage Valid Accuracy in epoch 14: 35.22000\n",
      "\tAverage Test Accuracy in epoch 14: 37.07000\n",
      "\n",
      "Average loss in epoch 15: 2.23644\n",
      "\tAverage Valid Accuracy in epoch 15: 39.32000\n",
      "\tAverage Test Accuracy in epoch 15: 41.71000\n",
      "\n",
      "Average loss in epoch 16: 2.21770\n",
      "\tAverage Valid Accuracy in epoch 16: 41.94000\n",
      "\tAverage Test Accuracy in epoch 16: 44.76000\n",
      "\n",
      "Average loss in epoch 17: 2.19057\n",
      "\tAverage Valid Accuracy in epoch 17: 43.26000\n",
      "\tAverage Test Accuracy in epoch 17: 45.64000\n",
      "\n",
      "Average loss in epoch 18: 2.15017\n",
      "\tAverage Valid Accuracy in epoch 18: 42.72000\n",
      "\tAverage Test Accuracy in epoch 18: 44.90000\n",
      "\n",
      "Average loss in epoch 19: 2.08892\n",
      "\tAverage Valid Accuracy in epoch 19: 42.60000\n",
      "\tAverage Test Accuracy in epoch 19: 44.53000\n",
      "\n",
      "Average loss in epoch 20: 2.00024\n",
      "\tAverage Valid Accuracy in epoch 20: 44.48000\n",
      "\tAverage Test Accuracy in epoch 20: 46.60000\n",
      "\n",
      "Average loss in epoch 21: 1.88498\n",
      "\tAverage Valid Accuracy in epoch 21: 48.74000\n",
      "\tAverage Test Accuracy in epoch 21: 50.07000\n",
      "\n",
      "Average loss in epoch 22: 1.74404\n",
      "\tAverage Valid Accuracy in epoch 22: 54.14000\n",
      "\tAverage Test Accuracy in epoch 22: 56.13000\n",
      "\n",
      "Average loss in epoch 23: 1.57319\n",
      "\tAverage Valid Accuracy in epoch 23: 60.18000\n",
      "\tAverage Test Accuracy in epoch 23: 61.81000\n",
      "\n",
      "Average loss in epoch 24: 1.37342\n",
      "\tAverage Valid Accuracy in epoch 24: 67.10000\n",
      "\tAverage Test Accuracy in epoch 24: 68.08000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_size = 28\n",
    "n_channels = 1\n",
    "n_classes = 10\n",
    "n_train = 55000\n",
    "n_valid = 5000\n",
    "n_test = 10000\n",
    "n_epochs = 25\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 # making sure Tensorflow doesn't overflow the GPU\n",
    "\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "if not os.path.exists('summaries'):\n",
    "    os.mkdir('summaries')\n",
    "if not os.path.exists(os.path.join('summaries','first')):\n",
    "    os.mkdir(os.path.join('summaries','first'))\n",
    "\n",
    "summ_writer = tf.summary.FileWriter(os.path.join('summaries','first'), session.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "accuracy_per_epoch = []\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_per_epoch = []\n",
    "    for i in range(n_train//batch_size):\n",
    "        \n",
    "        # =================================== Training for one step ========================================\n",
    "        batch = mnist_data.train.next_batch(batch_size)    # Get one batch of training data\n",
    "        if i == 0:\n",
    "            # Only for the first epoch, get the summary data\n",
    "            # Otherwise, it can clutter the visualization\n",
    "            l,_,gn_summ = session.run([tf_loss,tf_loss_minimize,tf_gradnorm_summary],\n",
    "                                      feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                                 train_labels: batch[1],\n",
    "                                                tf_learning_rate: 0.0001})\n",
    "            summ_writer.add_summary(gn_summ, epoch)\n",
    "        else:\n",
    "            # Optimize with training data\n",
    "            l,_ = session.run([tf_loss,tf_loss_minimize],\n",
    "                              feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                         train_labels: batch[1],\n",
    "                                         tf_learning_rate: 0.0001})\n",
    "        loss_per_epoch.append(l)\n",
    "        \n",
    "    print('Average loss in epoch %d: %.5f'%(epoch,np.mean(loss_per_epoch)))    \n",
    "    avg_loss = np.mean(loss_per_epoch)\n",
    "    \n",
    "    # ====================== Calculate the Validation Accuracy ==========================\n",
    "    valid_accuracy_per_epoch = []\n",
    "    for i in range(n_valid//batch_size):\n",
    "        valid_images,valid_labels = mnist_data.validation.next_batch(batch_size)\n",
    "        valid_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: valid_images.reshape(batch_size,image_size*image_size)})\n",
    "        valid_accuracy_per_epoch.append(accuracy(valid_batch_predictions,valid_labels))\n",
    "        \n",
    "    mean_v_acc = np.mean(valid_accuracy_per_epoch)\n",
    "    print('\\tAverage Valid Accuracy in epoch %d: %.5f'%(epoch,np.mean(valid_accuracy_per_epoch)))\n",
    "    \n",
    "    # ===================== Calculate the Test Accuracy ===============================\n",
    "    accuracy_per_epoch = []\n",
    "    for i in range(n_test//batch_size):\n",
    "        test_images, test_labels = mnist_data.test.next_batch(batch_size)\n",
    "        test_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: test_images.reshape(batch_size,image_size*image_size)}\n",
    "        )\n",
    "        accuracy_per_epoch.append(accuracy(test_batch_predictions,test_labels))\n",
    "        \n",
    "    print('\\tAverage Test Accuracy in epoch %d: %.5f\\n'%(epoch,np.mean(accuracy_per_epoch)))\n",
    "    avg_test_accuracy = np.mean(accuracy_per_epoch)\n",
    "    \n",
    "    # Execute the summaries defined above\n",
    "    summ = session.run(performance_summaries, feed_dict={tf_loss_ph:avg_loss, tf_accuracy_ph:avg_test_accuracy})\n",
    "\n",
    "    # Write the obtained summaries to the file, so it can be displayed in the Tensorboard\n",
    "    summ_writer.add_summary(summ, epoch)\n",
    "    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = \"../summaries/first/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=$logs_path --port 8890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "After this Notebook, you should know:\n",
    "- How to build an CNN network for an image classification task.\n",
    "- How to use Tensorboard to keep-track/monitoring the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
