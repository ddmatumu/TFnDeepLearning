{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with ELMO\n",
    "\n",
    "We want to predict sentiment from a IMDB movie review dataset. For every review we take only the first 100 words - we want in fact to check wheter most of the actual sentiment is at the beginning of the reviews, rather than at the end. \n",
    "\n",
    "In this dataset, sentiment is either positive or negative, 0 or 1. (There exists also a version with the 1 to 10 scale of votes)\n",
    "\n",
    "\n",
    "- [Tensorflow hub](https://www.tensorflow.org/hub): a hub of models\n",
    "- [ELMO](https://tfhub.dev/google/elmo/2) webpage\n",
    "\n",
    "\n",
    "\n",
    "### In this Notebook, you're going to finish some following #TODOs\n",
    "- Always try to read all of the code and try to understand it.\n",
    "- Excercise with early stopping and hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "def add_path(path):\n",
    "    module_path = os.path.abspath(os.path.join(path))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "add_path('../../pythonlibs/embeddings')\n",
    "add_path('../../pythonlibs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of these lines to install tensorflow_hub. Select to use either pip or conda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install tensorflow_hub\n",
    "#!conda install --yes --prefix {sys.prefix} numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0509 00:01:02.011122 140428235376448 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from imdb.helper import get_imdb_reviews_dataset\n",
    "from elmo.helper import get_elmo_embeddings_layer, transform_imdb_dataset, loss_pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n",
    "\n",
    " - see helper file for the function's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_folder = '../../../data/aclImdb'\n",
    "\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = get_imdb_reviews_dataset(path=dataset_folder, max_dataset_size=6000, trunc=100)\n",
    "\n",
    "\n",
    "x, y = np.vstack([train_x, test_x]), np.vstack([train_y, test_y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i am very sorry that this charming and whimsical film which i first saw soon after it was first released in the early fifties has had such a poor reception more recently in my opinion it has been greatly underrated  but perhaps it appeals more to the european sense of humour than to for example the american maybe we in europe can understand and appreciate its subtleties and situations more since we are closer to some of them in real life particular mention should be made of the limited but good music  especially the catchy and memorable song']\n",
      " ['xizao is a rare little movie it is simple and undemanding and at the same time so rewarding in emotion and joy the story is simple and the theme of old and new clashing is wonderfully introduced in the first scenes this theme is the essence of the movie but it would have fallen flat if it was not for the magnificent characters and the actors portraying themthe aging patriarch master liu is a relic of chinas preexpansion days he runs a bath house in an old neighbourhood every single scene set in the bath house is a source of']\n",
      " ['the godfather citizen kane star wars goodfellas none of the above compare to the complex brilliance of the sopranos each and every character has layers upon layers of absolute verity completely and utterly three dimensional we care about tony soprano wholeheartedly despite the fact that in the simplest model of good vs evil he is evil soprano is the most provocative intricate and fascinating protagonist ever created to this point in history if you are in the mood to be overtly challenged as a viewer and to be forever altered on your feelings toward entertainment watch the sopranos i defy']\n",
      " ...\n",
      " ['i guess i have still enough brain left to not find this movie funny great comedians  but a very poor movie the best performance still did nina hagen trivia did you realize that it the real world scenes in hamburg the cars are almost only new bmws  i guess i have still enough brain left to not find this movie funny great comedians  but a very poor movie the best performance still did nina hagen trivia did you realize that it the real world scenes in hamburg the cars are almost only new bmws ']\n",
      " ['this kind of inspirational saccharine is enough to make you sick it telegraphs its sentiments like the biggest semaphore on earth it removes from the audience its own interpretation and feeling by making the choices for it the big finish is swimming in weeping orchestration that must supposed to work like jumper cables on a dead car i guess you would need such prompting to feel if you are stupid enough to watch a film as simpleminded and sappy as this streep glows and you wonder if she really has the depth of feeling on display or if its just']\n",
      " ['please spare me of these movies that teach us that crime is fun and justified couple that with a vacuous script with an intense desire to be a farrelly or a coen brother plus the lives of yet another group of supposedly high school age people acting out their dawson creekbrand teen angst complete with a genxyz soundtrack that woefully tries to make the movie feel cool and we have intensely and painfully inept satirethis is not even watereddown ferris buelleri would rather watch a traffic light changeonly one scene stands out as anywhere near worth the price of admission']]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The dataset from (string, class) is transformed to (embedding, string, class) using ELMo\n",
    "\n",
    "\n",
    "ELMo is concatenated on the input placeholder.\n",
    "\n",
    "We will have: **Input(string) -> ELMo -> Output(float x 1024)**\n",
    "\n",
    "We use the 'default' output of ELMo, that for a sentence with tokens k1, k2, ... averages all of the word embeddings ELMok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 00:01:07.748224 140428235376448 deprecation.py:323] From /home/michele/Development/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "input_text = tf.placeholder(shape=(batch_size, ), dtype='string')\n",
    "elmo = get_elmo_embeddings_layer(input_text)\n",
    "\n",
    "\n",
    "'''\n",
    "We pass the dataset to ELMo for it to generate the latent representations of the reviews.\n",
    "'''\n",
    "\n",
    "sess.run([\n",
    "    tf.tables_initializer(), \n",
    "    tf.global_variables_initializer()\n",
    "])\n",
    "    \n",
    "    \n",
    "# Set load = False to generate the embeddings with ELMO. It takes a lot of time.  \n",
    "# Set load = True to load a pre-precessed dataset (given) from the hard-disk.\n",
    "embs = transform_imdb_dataset(sess, input_text, elmo, x, batch_size, load=True, npy_path='./imdb_embs.npy')\n",
    "\n",
    "\n",
    "assert len(embs) == len(x) == len(y)\n",
    "\n",
    "# embs, x and y are row-aligned\n",
    "\n",
    "\n",
    "perc_train = .8\n",
    "\n",
    "\n",
    "row_split = int(perc_train * len(x))\n",
    "\n",
    "\n",
    "# We shuffle the dataset\n",
    "randind = np.random.permutation(x.shape[0])\n",
    "\n",
    "x = x[randind]\n",
    "y = y[randind, :]\n",
    "embs = embs[randind, :]\n",
    "\n",
    "\n",
    "train_embed, train_labels, train_values = embs[:row_split, :], y[:row_split], x[:row_split, :]\n",
    "test_embed, test_labels, test_values    = embs[row_split:, :], y[row_split:], x[row_split:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Specify a classifier that will use ELMo embeddings\n",
    "\n",
    "It is a standard feed-forward network as we don't directly concatenate it to ELMo. \n",
    "\n",
    "We decided to separate the classifier from the generation of the representations to speed-up the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 00:01:11.026981 140428235376448 deprecation.py:323] From <ipython-input-7-bbe967b1c85d>:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0509 00:01:11.057341 140428235376448 deprecation.py:323] From <ipython-input-7-bbe967b1c85d>:9: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0509 00:01:11.062977 140428235376448 deprecation.py:506] From /home/michele/Development/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Specifications of the classifier\n",
    "\n",
    "input_embed  = tf.placeholder(dtype='float', shape=(None, 1024), name='input_emb')\n",
    "input_labels = tf.placeholder(dtype='float',  shape=(None, 2), name='input_lbl')\n",
    "\n",
    "dense1 = tf.layers.dense(input_embed, 500, activation='sigmoid')\n",
    "\n",
    "prob = tf.placeholder_with_default(.0, shape=())\n",
    "dout1  = tf.layers.dropout(dense1, rate=prob, training=True)\n",
    "\n",
    "output = tf.layers.dense(dout1, 2, activation='linear')\n",
    "\n",
    "\n",
    "# Loss function and optimization criteria\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=input_labels)\n",
    "opt = tf.train.RMSPropOptimizer(1e-3)\n",
    "opt_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model\n",
    "\n",
    "Using above defined network and the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:      1/20     Train loss: 0.7266153552134832 Test loss: 0.7168813848495483\n",
      "Epoch:      2/20     Train loss: 0.6692285135388374 Test loss: 0.6414126340548197\n",
      "Epoch:      3/20     Train loss: 0.6419026438395182 Test loss: 0.6295867749055226\n",
      "Epoch:      4/20     Train loss: 0.629452684322993 Test loss: 0.6258252811431885\n",
      "Epoch:      5/20     Train loss: 0.6201454809308052 Test loss: 0.6236344202359517\n",
      "Epoch:      6/20     Train loss: 0.6113301544388136 Test loss: 0.6226768004894256\n",
      "Epoch:      7/20     Train loss: 0.602236191034317 Test loss: 0.6225941967964173\n",
      "Epoch:      8/20     Train loss: 0.5929680331548055 Test loss: 0.6227985227108002\n",
      "Epoch:      9/20     Train loss: 0.5836559851964315 Test loss: 0.6231621913115184\n",
      "Epoch:     10/20     Train loss: 0.5741625733176867 Test loss: 0.6238262462615967\n",
      "Epoch:     11/20     Train loss: 0.5643218252062797 Test loss: 0.6248316756884257\n",
      "Epoch:     12/20     Train loss: 0.5539860497911772 Test loss: 0.6261344528198243\n",
      "Epoch:     13/20     Train loss: 0.5430442435542743 Test loss: 0.6277341020107269\n",
      "Epoch:     14/20     Train loss: 0.5314254334568977 Test loss: 0.6296436731020609\n",
      "Epoch:     15/20     Train loss: 0.5190802951653798 Test loss: 0.6318174974123637\n",
      "Epoch:     16/20     Train loss: 0.5060214361051718 Test loss: 0.634155912399292\n",
      "Epoch:     17/20     Train loss: 0.4923357520500819 Test loss: 0.6365410772959391\n",
      "Epoch:     18/20     Train loss: 0.47809667224685354 Test loss: 0.6389740975697835\n",
      "Epoch:     19/20     Train loss: 0.46333717038234074 Test loss: 0.6414521984259287\n",
      "Epoch:     20/20     Train loss: 0.4480513489743074 Test loss: 0.6440182566642761\n"
     ]
    }
   ],
   "source": [
    "# Now we train the model!\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Feed dictionaries\n",
    "train_feeds = {input_embed: train_embed, input_labels: train_labels}\n",
    "test_feeds  = {input_embed: test_embed,  input_labels: test_labels}\n",
    "\n",
    "\n",
    "sess.run([\n",
    "    tf.tables_initializer(), \n",
    "    tf.global_variables_initializer()\n",
    "])\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    train_loss = loss_pass(train_feeds, batch_size, sess, loss, train_op=opt_op, log=False)\n",
    "    test_loss  = loss_pass(test_feeds,  batch_size, sess, loss)\n",
    "\n",
    "    print('Epoch: {:>6}/{:<6} Train loss: {} Test loss: {}'.format(e+1, epochs, train_loss, test_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict\n",
    "\n",
    "Now some predictions of the test set.\n",
    "- Can you guess which class is what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cb9f3106f06f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "probabilities = tf.nn.softmax(output)\n",
    "\n",
    "n_show = 5\n",
    "for i, r in enumerate(test_x):\n",
    "    \n",
    "    if i > n_show:\n",
    "        break\n",
    "    \n",
    "    result= sess.run(probabilities, \n",
    "                     feed_dict= {\n",
    "                         input_embed: test_embed[i,:].reshape(1,-1),\n",
    "                         prob: .0\n",
    "                    })\n",
    "    \n",
    "    print(f'{str(test_values[i])} \\n Class 0: {result[0][0]} Class 1: {result[0][1]}')\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exercises\n",
    "\n",
    "\n",
    "### Early stopping\n",
    "\n",
    "Design an early stopping criteria and implement it.\n",
    "\n",
    "\n",
    "#### From wiki:\n",
    "\n",
    "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set (*the test set*). Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation. \n",
    "\n",
    "\n",
    "### Hyperparameter search\n",
    "Design an algorithm that bruteforces all of the combinations of the selected hyperparameters. Keeping only the combination with best performance\n",
    "- Select some parameters, such as layers size, learning rate, dropout\n",
    "- Select for each parameter min value, max value, step\n",
    "- Run into nested loops the training procedure, sturing the final loss\n",
    "\n",
    "\n",
    "#### From wiki:\n",
    "\n",
    "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned. \n",
    "\n",
    "\n",
    "### Can you draw any conclusion on the efficacy of the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusions\n",
    "\n",
    "This Notebook containes code to load a dataset, process it with ELMo, and run a classifier with the obtained representations. This is transfer learning in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
